these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited generative ai and llms snowflake special edition by david baum these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited generative ai and llms for dummies snowflake special edition published by john wiley sons inc river st hoboken nj www wiley com copyright by john wiley sons inc hoboken new jersey no part of this publication may be reproduced stored in retrieval system or transmitted in any form or by any means electronic mechanical photocopying recording scanning or otherwise except as permitted under sections or of the united states copyright act without the prior written permission of the publisher requests to the publisher for permission should be addressed to the permissions department john wiley sons inc river street hoboken nj fax or online at http www wiley com go permissions trademarks wiley for dummies the dummies man logo the dummies way dummies com making everything easier and related trade dress are trademarks or registered trademarks of john wiley sons inc and or its affiliates in the united states and other countries and may not be used without written permission snowflake and the snowflake logo are trademarks or registered trademarks of snowflake inc all other trademarks are the property of their respective owners john wiley sons inc is not associated with any product or vendor mentioned in this book limit of liability disclaimer of warranty while the publisher and authors have used their best efforts in preparing this work they make no representations or warranties with respect to the accuracy or completeness of the contents of this work and specifically disclaim all warranties including without limitation any implied warranties of merchantability or fitness for particular purpose no warranty may be created or extended by sales representatives written sales materials or promotional statements for this work the fact that an organization website or product is referred to in this work as citation and or potential source of further information does not mean that the publisher and authors endorse the information or services the organization website or product may provide or recommendations it may make this work is sold with the understanding that the publisher is not engaged in rendering professional services the advice and strategies contained herein may not be suitable for your situation you should consult with specialist where appropriate further readers should be aware that websites listed in this work may have changed or disappeared between when this work was written and when it is read neither the publisher nor authors shall be liable for any loss of profit or any other commercial damages including but not limited to special incidental consequential or other damages for general information on our other products and services or how to create custom for dummies book for your business or organization please contact our business development department in the at contact info dummies biz or visit www wiley com go custompub for information about licensing the for dummies brand for products or services contact brandedrights licenses wiley com isbn pbk isbn ebk publisher acknowledgments some of the people who helped bring this book to market include the following development editor nicole sholly project manager jennifer bingham acquisitions editor traci martin editorial manager rev mengle sales manager molly daugherty content refinement specialist saikarthick kumarasamy table of contents iii these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited table of contents introduction about this book icons used in this book beyond the book chapter introducing gen ai and the role of data the historical context of gen ai introducing llms and foundation models transforming the ai landscape accelerating ai functions the role of data in ai projects explaining the importance of generative ai to the enterprise pretrained models security versus ease of use managing gen ai projects with cloud data platform chapter understanding large language models categorizing llms defining general purpose llms using task specific and domain specific llms reviewing the technology behind llms introducing key terms and concepts explaining the importance of vector embeddings identifying developer tools and frameworks enforcing data governance and security extending governance for all data types chapter llm app project lifecycle defining the use case and scope selecting the right llm comparing small and large language models adapting llms to your use case engineering prompts learning from context augmenting text retrieval fine tuning language models reinforcement learning using vector database iv generative ai and llms for dummies snowflake special edition these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited implementing llm applications deploying apps into containers allocating specialized hardware integrating apps and data chapter bringing llm apps into production adapting data pipelines semantic caching feature injection context retrieval processing for inference reducing latency calculating costs creating user interfaces simplifying development and deployment orchestrating ai agents chapter reviewing security and ethical considerations reiterating the importance of security and governance centralizing data governance alleviating biases acknowledging open source risks contending with hallucinations observing copyright laws chapter five steps to generative ai identify business problems select data platform build data foundation create culture of collaboration measure learn celebrate introduction these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited introduction enerative ai gen ai and large language models llms are revolutionizing our personal and professional lives from supercharged digital assistants that manage our email to seemingly omniscient chatbots that can communicate with enterprise data across industries languages and specialties these technologies are driving new era of convenience produc tivity and connectivity in the business world gen ai automates huge variety of menial tasks saving time and improving efficiency it generates code aids in data analysis and automates content creation freeing knowledge workers to focus on critical and creative tasks it also enhances personal experiences by tailoring content to your pref erences delivering personalized recommendations for playlists movies and news feeds that enrich our daily lives traditional ai uses predictive models to classify data recognize patterns and predict outcomes within specific context or domain such as analyzing medical images to detect irregularities gen ai models generate entirely new outputs rather than simply making predictions based on prior experience this shift from prediction to creation opens up new realms of innovation for example while traditional predictive model can spot suspicious lesion in an mri of lung tissue gen ai app can also determine the likelihood that patient will develop pneumonia or some other type of lung disease and offer treatment recommendations based on best prac tices gleaned from thousands of similar cases both in the public sphere of the internet and within the realm of private enterprise the transformative potential of this rapidly evolving field is reshaping the way people live work and interact about this book this book provides an introductory overview to llms and gen ai applications along with techniques for training tuning and deploying machine learning ml models the objective is to pro vide technical foundation without getting into the weeds and to help bridge the gap between ai experts and their counterparts in marketing sales finance product and more generative ai and llms for dummies snowflake special edition these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited in the pages that follow you learn about the importance of gen ai applications that are secure resilient easy to manage and that can integrate with your existing technology ecosystem you also discover the importance of standardizing on modern data plat form to unlock the full potential of your data prepare to embark on transformative journey that will shape the way your business operates icons used in this book throughout this book the following icons highlight tips impor tant points to remember and more tips guide you to easier ways to perform task or better ways to use gen ai in your organization this icon highlights concepts worth remembering as you immerse yourself in the understanding and application of gen ai and llm principles the jargon beneath the jargon explained beyond the book if you like what you read in this book and want to know more visit www snowflake com where you can learn about the company and what it offers try snowflake for free obtain details about different plans and pricing view webinars access news releases get the scoop on upcoming events access documentation and get in touch with them they would love to hear from you disclaimer snowflake ai features and capabilities that are refer enced or described in this book may not be generally available be different than described or no longer exist at the time of reading chapter introducing gen ai and the role of data these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited chapter in this chapter reviewing the history of ai emphasizing the role of data in gen ai projects discussing the importance of gen ai to the enterprise using cloud data platform to manage gen ai initiatives introducing gen ai and the role of data raditional ai often referred to as machine learning ml has primarily focused on analytic tasks like classification and prediction generative ai gen ai goes step further with its ability to create new original content this creative breakthrough has the potential to transform nearly every indus try enhancing human creativity and pushing the boundaries of what machines can accomplish this chapter puts gen ai in his torical context defines key terms and introduces the data foun dation that organizations need to succeed with gen ai initiatives the historical context of gen ai gen ai is type of artificial intelligence that uses neural networks and deep learning algorithms to identify patterns within exist ing data as basis for generating original content by learning patterns from large volumes of data gen ai algorithms synthe size knowledge to create original text images audio video and other forms of output to understand the transformative nature generative ai and llms for dummies snowflake special edition these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited of these unique technologies it is helpful to place them in their historical context ai has rich history marked by decades of steady progress occasional setbacks and periodic breakthroughs although certain foundational ideas in ai can be traced back to the early th century classical or traditional ai which focused on rule based systems had its inception in the and came into prominence in the ensuing decades ml which involves train ing computer algorithms to learn patterns and make predictions based on data emerged in the at about this same time neural networks gained popularity inspired by the structure and functioning of the human brain these software systems use interconnected nodes neurons to process information during the first two decades of the st century deep learning revolutionized the ai landscape with its capability to handle large amounts of data and execute complex tasks as type of neural network deep learning employs multiple layers of interconnected neurons allowing for more sophisticated learning and represen tation of data this breakthrough led to significant advancements in computer vision speech recognition and natural language processing nlp launching the era of general purpose ai bots such as siri and alexa convolutional neural networks cnns proved themselves to be particularly successful at computer vision tasks while recurrent neural networks rnns excelled in sequential data processing such as language modeling these technologies laid the foundation for gen ai introducing llms and foundation models large language models llms are advanced ai systems designed to understand the intricacies of human language and to generate intelligent creative responses when queried successful llms are trained on enormous data sets typically measured in petabytes million gigabytes training data has often been sourced from books articles websites and other text based sources mostly in the public domain using deep learning techniques these models excel at understanding and generating text similar to human produced content today llms power many modern applica tions including content creation tools language translation apps customer service chatbots financial analysis sites scientific research repositories and advanced internet search tools chapter introducing gen ai and the role of data these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited in the field of ai language models are powerful software systems designed to understand generate and manipulate human lan guage some models handle images and other media along with text these are often referred to as multimodal language models transforming the ai landscape ai systems with humanlike reasoning capabilities have been around since the but only with the advent of llms have they gained widespread adoption according to recent forbes article called transformers revolutionized ai what will replace them key breakthrough came in when the google brain team introduced the transformer architecture deep learning model that replaced traditional recurrent and convolutional structures with new type of architecture that particularly effective at understanding and contextualizing language as well as generat ing text images audio and computer code llms based on the transformer architecture have enabled new realms of ai capabilities perhaps the best known example is openai chatgpt which stands for chatbot generative pre trained transformer cnn article microsoft confirms it investing billions in the creator of chatgpt shows support for the development of progressively larger llms some of which may incorporate hundreds of billions of parameters to generate coherent and contextually relevant responses accelerating ai functions another important factor in the evolution of ai is the advent of accelerated hardware systems known as graphics processing units gpus although central processing units cpus are designed for general purpose computing tasks gpus initially developed for graphics rendering are specialized processors that have proven to be adept at ml tasks due to their unique architecture gpus have large number of cores that can process multiple tasks simultaneously transformers use gpus to process multiple threads of information leading to faster training of ai models that effectively handle not just text but also images audio and video content this parallel processing capability is crucial for the computationally intensive calculations involved in ml such as generative ai and llms for dummies snowflake special edition these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited matrix operations gpus can perform these computations much faster than cpus accelerating training and inference times and enhancing the overall performance of ml algorithms refer to cloud data science for dummies wiley by david baum for addi tional information on these concepts figure summarizes ai progress the role of data in ai projects as impressive as they are at language generation reasoning and translation gen ai applications that have been built on public data can realize their full potential in the enterprise until they re cou pled with enterprise data stores most organizations store massive amounts of data both on premises and in the cloud many of these businesses have data science practices that leverage structured data for traditional analytics such as forecasting to maximize the value of gen ai these companies need to open up to the vast world of unstructured and semistructured data as well according to february report from mit titled tapping the power of unstructured data to percent of data is unstructured locked away in text audio social media and other sources for enterprises that figure out how to use this data it can provide competitive advantage especially in the era of gen ai to amass complete data set consider not only your internal first party data but also second party data from partners and suppliers and third party data from service provider or data marketplace see the nearby sidebar for more information figure gen ai builds on traditional ai concepts while vastly expanding applicability scaling potential and with web scale processing demands chapter introducing gen ai and the role of data these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited explaining the importance of generative ai to the enterprise today llms have paved the way for an immense array of advanced applications centered around content generation logi cal reasoning language translation text retrieval code genera tion content summarization and search llms for content generation gen ai can streamline content creation by generating various types of media including text sound and images for instance marketing department can utilize gen ai to generate the first drafts of blogs press releases posts on formerly twitter and product descriptions including producing custom images for promotional campaigns one popular use of this technology in the enterprise is to develop chatbots that engage in conversational interactions with business users helping them obtain accurate answers to their questions by harnessing private data such as customer transaction histories and customer service records these systems can even deliver personalized content to target audiences while maintaining data security llms are also adept at analyzing documents summarizing unstructured text and converting unstructured text into structured table formats cast wide data net to maximize the potential of your gen ai endeavors cast wide net to utilize the three basic types of data sources first party data is internal data produced via everyday business interactions with customers and prospects second party data is produced by or in collaboration with trusted partners such as product inventory data shared with an commerce or retail sales channel third party data can be acquired from external sources to enrich internal data sets common examples include manufacturing sup ply chain data and financial market data generative ai and llms for dummies snowflake special edition these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited llms as logical reasoning engines within the field of ai natural language understanding nlu focuses on compre hending the intricate meaning in human communication llms can unravel the underlying meaning in textual data such as product reviews social media posts and customer surveys this makes them valuable for sentiment analysis and other complex reasoning tasks that involve extracting meaningful insights from text and providing deeper understanding of human language llms as translation engines llms have transformed text translation between languages making it easier for people to communicate across linguistic barriers by leveraging this understanding llms can accurately convert text from one language to another ensuring effective and reliable transla tion this breakthrough in language processing has greatly enhanced accessibility and global communication allowing individuals and businesses to connect collaborate and understand each other more easily regardless of language differences llms for text retrieval summarization and search llms are pretrained on vast amounts of text data allowing them to grasp the nuances of language and comprehend the meaning of text they can search through large databases or the internet in general to locate relevant information based on user defined queries llms can also generate concise summaries while maintaining the essence of the original information for example tech company might use an llm to optimize content for search engines by suggesting relevant keywords giving visibility into common search queries associated with the topic and ensuring crawlability gen ai models and hence the decisions made from those models are only as good as the data that supports them the more data these models ingest and the more situations they encounter the smarter and more comprehensive they become pretrained models there rapidly growing market for creating and customiz ing gen ai foundation models in many different industries and domains this has given rise to surge of llms that have been pretrained on data sets with millions or even billions of records chapter introducing gen ai and the role of data these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited allowing them to accomplish specific tasks for example as explained by siliconangle nvidia debuts new ai tools for bio molecular research and text processing megamolbart part of the nvidia bionemo service and framework can understand the language of chemistry and learn the relationships between atoms in real world molecules giving researchers powerful tool for faster drug discovery pharmaceutical companies can fine tune these foundation models using their own proprietary data train ing these commercial foundation models is an immense effort that costs tens of millions of dollars fortunately businesses that use them don have to repeat that massive process to adapt an llm to their needs they can adapt an existing foundation model for fraction of that amount large technology companies are constantly inventing new model architectures even as they expand the capabilities of their exist ing llms for more on this see chapter thousands of open source models are available on public sites such as github and hugging face developers can use the pretrained ai models as foundation for creating custom ai apps security versus ease of use all logical reasoning engines need data to function although many of today llms have been trained on vast amounts of internet data they become even more powerful and relevant when they re trained with enterprise data because much of this data is pro tected in private databases and resides behind network firewalls the challenge facing today enterprises involves augmenting llms with this corporate data in secure and governed manner gen ai systems learn from data the more data they can access the more capable they become but how do you ensure that your business users software developers and data scientists can easily access secure consistent governed data set without adding onerous constraints that inhibit innovation enterprises need to be able to leverage gen ai technology in an easy straightforward manner they also need to uphold essential data security gov ernance and regulatory issues not only for their data but also for the models that learn from the data and extract information from it how can you achieve this without squelching innovation you start by unifying data in comprehensive repository that multiple generative ai and llms for dummies snowflake special edition these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited workgroups can access easily and securely this allows you to centralize data governance and democratize access to gen ai ini tiatives across your organization while minimizing complexity and optimizing costs managing gen ai projects with cloud data platform cloud data platform is specialized cloud service optimized for storing analyzing and sharing large and diverse volumes of data it unifies data security and data governance activities by ensuring that all users leverage single copy of data it fosters collaboration and ensures that the organization has scalable data environ ment for new foundation models and related analytic endeavors cloud data platform extends your ai horizons by allowing you to store your first party data and leverage network of data from second and third party data providers as well it provides pro tected ecosystem where you can easily and securely share models and data sets internally and with partners and customers by utilizing cloud data platform you can seamlessly leverage existing infrastructure to support gen ai initiatives with mini mal hassle as fully managed service the platform eliminates the need to deal with the complexities and technical overhead of building and managing infrastructure you can easily provi sion and effortlessly scale compute resources for each type of data such as gpus for model training fine tuning and infer ence activities finally by using the same core data foundation for all your data driven initiatives you can ensure consistency and reliability in managing your gen ai data science and ana lytics projects data is your core differentiator in the age of gen ai the best way to harness and protect enterprise data for gen ai initiatives is to consolidate disparate sources into cloud data platform that provides strong security and governance for data and the models customized with that data data can be structured tabular data semistructured data from iot devices weblogs and other sources or unstructured data such as image files and pdf documents chapter understanding large language models these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited chapter in this chapter categorizing and classifying llms reviewing the technologies that power llms understanding the role of vector databases identifying llm terms concepts and frameworks reiterating the importance of data governance understanding large language models arge language models llms are widely known for their ability to generate written text computer code and other content as well as for their astonishing ability to respond to queries in humanlike ways however the utility of these ai sys tems extends beyond explaining concepts and summarizing text today llms have the potential to revolutionize how enterprises acquire handle and analyze information opening up new ave nues for exploration and inquiry this chapter defines the various types of llms and discusses their applicability to the enterprise categorizing llms general purpose llms handle wide range of tasks and under stand broad spectrum of languages both natural languages and computer languages they are trained by scraping massive amounts of data from the internet as well as by ingesting data generative ai and llms for dummies snowflake special edition these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited from private data sources that are relevant to the purpose of the model this allows llms to generate contextually related feed back on just about any topic foundation models are class of generative ai gen ai models that are well suited for wide range of use cases to increase their usefulness at specific task these models can be specialized trained or modified for specific applications common founda tion models include the following task specific llms such as meta code llama specialize in unique highly targeted tasks like generating software code domain specific llms apply gen ai technology to specific subjects and industries for example nvidia biobert which has been trained on biomedical text helps research ers understand scientific literature and extract information from medical documents domain specific and task specific models are fine tuned using data specific to the domain they re built for such as law medi cine cybersecurity art and countless other fields they aren limited to language some of them can also generate music pic tures video and other types of multimodal content an llm is general purpose model primarily useful for tasks related to unstructured text data foundation model serves as the basis for developing specialized applications adapted to specific industries business problems and use cases foundation model can often be multimodal meaning it handles both text and other media such as images defining general purpose llms gpt general purpose llm was developed by openai based on the generative pre trained transformer gpt series of machine learning ml models chatgpt isn language model per se but rather user interface tailored around particular lan guage model such as gpt gpt or gpt all of which have been optimized for conversational interactions within the chat gpt llm platform other popular llm options are described in the nearby sidebar chapter understanding large language models these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited those listed in the sidebar and other language models are becom ing progressively more relevant to the business world according to june press release from bloomberg intelligence the gen ai market is poised to explode growing to trillion over the next years from market size of just billion in compound annual growth of percent sizing up the contenders as the software industry steps up research and development into llms several prominent offerings have emerged in this highly competitive sector openai gpt family is based on the gpt series of models these llms are renowned for their impressive language generation capabilities and capability to perform well across various language tasks including search text generation reasoning and multime dia content delivery bidirectional encoder representations from transformers bert developed by google employs masked language model to learn contextual representations enabling it to better compre hend the meaning of sentences this model powers google bard conversational ai chat service llama large language model meta ai is family of llms intro duced by meta that excels at language translation text generation and question answering llama is an open source model that is available for research and development purposes code llama also developed by meta ai is language model tailored to understand and generate code snippets and program ming instructions it has been trained to assist in coding tasks code completion and suggesting efficient coding techniques snowflake copilot an llm fine tuned by snowflake generates sql from natural language and refines queries through conversa tion improving user productivity xlnet developed by carnegie mellon university and google focuses on generating high quality text in multiple languages making it useful for language translation and content creation generative ai and llms for dummies snowflake special edition these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited using task specific and domain specific llms bing and bard are examples of applications developed utiliz ing their respective foundation llms these applications have user interface and have undergone additional specialized training enhancing their capabilities for specific tasks for example bard offers chatbot access to google full suite of products includ ing youtube google drive google flights and others to assist users in wide variety of tasks google users can link their personal gmail google docs and other account data to allow bard to analyze and manage their personal information for example you can ask bard to plan an upcoming trip based on suggestions from recent email string complete with flight options you can also ask bard to summarize meeting notes you have logged in the files and folders of your google drive hierarchy domain specific llms focus on specific subject area or indus try for example biobert is trained on biomedical text mak ing it an excellent resource for understanding scientific literature and extracting information from medical documents codebert is cybersecurity solution that has been trained to assist with it security concerns such as vulnerability detection code review and software security analysis these specialized llms can be further trained and fine tuned using data specific to targeted areas of interest and can incorporate additional sources of data to build proficiency on designated subjects to gain adoption and drive value from models ai teams must build user interfaces that allow users to interact with these llms in designated ways reviewing the technology behind llms neural networks are key component of ai systems as discussed in the previous chapter most neural networks use combination of complex recurrent or cnn structures to do their jobs however today gen ai models also have an attention mechanism that helps the encoder the part that understands the input and the decoder the part that generates the output work together more effectively see figure chapter understanding large language models these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited it all springs from the advent of the transformer architecture introduced in chapter which presented new and simpler way to understand language using attention mechanisms unlike the previous models transformer models eliminate dependencies on other processing steps giving them the added benefit of being more parallelizable which accelerates training the power of the transformer architecture lies in its capability to learn the relevance of each word to all the other words in state ment or document through self attention mechanisms in con junction with training on large data sets introducing key terms and concepts today llms are easy to use rather than requiring formal code to communicate with software libraries and application program ming interfaces apis they can understand natural language or human instructions and perform tasks similar to human the text you provide to language model is called prompt the prompt is given to the model which then generates an answer the result produced by the model is known as completion and the process of using the model to generate text is called inference as you will see in chapters and users can influence the learning process by providing well crafted prompts or by using techniques such as reinforcement learning with human feedback rlhf to guide the model output you ll also learn about prompt engineering and in context learning icl which allow you to guide the model at inference time as well as how to figure gen ai encoder and decoder work together to produce accurate outputs generative ai and llms for dummies snowflake special edition these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited fine tune input parameters as you instruct the llm to generate relevant outputs specific to your private data explaining the importance of vector embeddings llms deal with large and complex data sets by representing these data sets in vector form where words are represented by num bers in multidimensional space it becomes easier for the mod els to compare and analyze information vector databases store data as mathematical representations that can be easily parsed by ml models these vector embeddings are the most efficient way to process and store quantitative representations of natural language data can be identified based on similarity metrics such as distance between two vectors instead of exact matches which saves tremendous amount of processing time see figure vector databases enable gen ai systems to quickly retrieve rel evant data during the generation and inference processes they are particularly useful for ml models because of their capability to power key language applications such as search recommenda tions and text generation make sure that your data platform includes vector search func tionality that can handle essential gen ai tasks that help you contextualize llms with your data such as retrieval augmented generation rag in context learning icl and vector similar ity search vss for more information on these concepts see chapter figure word can be identified by the group of words that are used with it frequently this is the basis for creating vector embeddings chapter understanding large language models these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited identifying developer tools and frameworks the capability of llms to process and interpret vast amounts of text audio video and other forms of content have made them an indispensable part of many data science workflows although nontechnical users can interact with llms with little or no train ing data science teams use established software frameworks to interact with llms and create gen ai applications popular frame works that assist with text classification sentiment analysis text generation and other gen ai tasks include the following openai gpt playground openai provides pretrained language models such as gpt which can be accessed via apis for various language understanding and text generation tasks the gpt playground allows users to experiment with these models and fine tune prompts interactively snowflake cortex an intelligent fully managed service that offers access to industry leading ai models llms and vector search functionality on secure and governed enterprise data hugging face transformer library an open source library that offers high level api for working with pretrained language models like bert and gpt the library includes user friendly interface and prebuilt functionality for fine tuning llms with straightforward apis and command line tools to simplify the process enforcing data governance and security models need abundant data relating to the problems they re attempting to solve but how do you put the right data in the hands of llm users without exposing sensitive information compromising data privacy or putting your brand at risk comprehensive cloud data platform allows you to work with llms within protected environment rather than taking your data to the processing engine it allows you to bring your pro cessing to the data where you can control user access to corpo rate data sources and enforce security and governance policies if you can run that model as service within your cloud data plat form you can ensure that data prompts and completions are not shared with unauthorized users generative ai and llms for dummies snowflake special edition these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited all gen ai project stakeholders should take care to protect sensi tive data as it is accessed shared and exchanged with llms and gen ai applications when you conduct these projects within cloud data platform you can uphold data security data privacy and data governance without imposing intrusive restrictions on the workforce extending governance for all data types modern cloud data platform extends governance to all types of data structured semistructured and unstructured this is unique capability not necessarily offered by cloud service object stores such as amazon microsoft azure blob storage and google cloud storage these cloud services make it relatively easy to store unstructured data such as pdf files audio and video as binary large objects blobs however granular access controls such as row level permissions aren always available at the blob level these services may broaden your access to complex data types but not without increasing risk even if you choose to store your data in cloud object store be sure that your cloud data platform can provide governance on top of that data loading all data into centralized repository with cohesive layer of data governance services allows you to enforce universal policies that broaden access while reducing risk applying these controls in modern cloud data platform that supports structured data semistructured data and unstructured data is easier and less risky chapter llm app project lifecycle these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited chapter in this chapter defining the scope of the project selecting an appropriate llm adapting llms to particular tasks exploring prompt engineering fine tuning llm models and more exposing data and llms as applications llm app project lifecycle he generative ai gen ai project lifecycle guides you through the process of selecting adapting and implement ing large language models llms as you create ai applica tions this chapter describes the major steps including defining the use case selecting an llm and guiding the use and customization of the model for the project it also covers the key considerations in model customization steps defining the use case and scope the first step in the gen ai project lifecycle see figure is to identify the business problem or use case for example you might want to use gen ai to create personalized product descriptions summarize transcripts extract answers from documents create compelling characters for video game or to train computer vision system to recognize particular objects generative ai and llms for dummies snowflake special edition these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited next determine what proprietary data you will use to custom ize or contextualize the model effectively many foundation llms contain massive amounts of information learned from the inter net which gives the models their knowledge of language as well as many aspects of the world around us more than likely you have project in mind that requires specific domain knowledge or access to internal information for example to create product descriptions for an commerce system you might begin with public data that describes the general types of products supple mented by internal data that identifies the unique aspects of your products defining the use case sets the foundation for clarifying the business problem and or the goal to be achieved it will help define data and user experience requirements selecting the right llm many different language models are available for public use for more on this see chapter hosted llms such as chat gpt and bard are provided as service that anybody can access via user interface or via apis this makes interacting with the llms very easy because there no overhead to host and scale the infrastructure where it runs open source llms like llama are freely available for download and modification and you can deploy them in your own environment although this gives you more control it also requires you to set up and maintain the underlying infrastructure not sending data to an external envi ronment and having more control over the model may be of high importance for sensitive data but the additional control puts the compute infrastructure management in your hands figure the gen ai project lifecycle and its players at glance chapter llm app project lifecycle these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited comparing small and large language models the parameters in language model refer to the trainable vari ables more parameters mean more knowledge is part of the model out of the box but bigger isn always better smaller llms have fewer parameters and thus consume less compute resources and are faster to fine tune and deploy they re well suited for running very specific tasks in more cost effective way llms have higher number of parameters typically billion or more and can learn more nuanced language patterns and provide more accurate and contextually relevant outputs for wider range of scenarios however they require more resources to train and adapt to your needs for example with only million parameters gpt is good choice for narrow set of tasks such as language completion and summarization with billion parameters gpt is better for complex tasks such as translating text and generating dialogue the choice between small and large language models is cost performance tradeoff it all depends on the set of use cases that need to be supported by single model selection criteria for llms when selecting the right llm for gen ai project consider task alignment choose an llm that aligns to the task such as gpt for conversational applications or biobert for biomedical research training data evaluate whether the llm has been trained on data that matches the domain or context of the project model size and complexity models with tens of billions of parameters provide higher quality outputs but require more computational resources adapting and tuning determine if the chosen llm can be effectively contextualized with prompts or fine tuning ecosystem and support investigate the availability of resources tools and community support surrounding the llm generative ai and llms for dummies snowflake special edition these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited adapting llms to your use case this list highlights various techniques that you can use to tailor the llm to meet your specific needs prompt engineering is the process of optimizing text prompts to guide the llm to generate pertinent responses in context learning icl allows the model to dynamically update its understanding during conversation resulting in more contextually relevant responses retrieval augmented generation rag combines retrieval and generation models to surface new relevant data as part of prompt fine tuning entails customizing pretrained llm to enhance its performance for specific domains or tasks reinforcement learning from human feedback rlhf is the ongoing approach to fine tuning in near real time by providing the model with feedback from human evaluators who guide and improve its responses each of these techniques is described in further detail in the following sections engineering prompts llms are sophisticated predictive models that anticipate the next word in sequence based on the context provided to them as part of process referred to as completion carefully constructed prompts help these models deliver tailored content yielding better completions llm performance is influenced not only by the training data but also by the context provided by these user inputs the prompts prompt engineering is the practice of crafting inputs to shape the output of language model and achieve desired result for instance if you re using the llm to generate summary of page research paper you can engineer the prompt by speci fying which sections of the document are most important and what should be the word count of the output by carefully crafting the prompt you can obtain more targeted and relevant responses zero shot one shot and few shot prompting are all techniques used in prompt engineering chapter llm app project lifecycle these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited zero shot prompting is the default you simply issue question and rely on the llm pretrained information to answer it with one shot prompting you include an example of the desired output to help the model understand the desired output for instance assume you re writing travel brochure and you want the ai model to describe vibrant public market in an exotic city even with limited exposure to this particular scenario model can generate creative descrip tion that matches the tone of voice vibrant use of adjectives and structure that has already proven successful in your marketing content few shot prompting takes it step further by providing multiple examples to more clearly teach the llm the desired output structure and language prompt engineering involves carefully crafting prompts to coax the language model toward desired outcome based on explicit instructions and context carefully constructed prompt will help ensure clarity provide insight to influence model performance learning from context although prompt engineering involves carefully designing and refining instructions to help the llm formulate useful and accurate completion in context learning icl involves training language model with data set that aligns with the desired context or domain by exposing the model to contextual information such as relevant documents or proprietary content it becomes better equipped to generate accurate and coherent responses within that context for example you could use icl to train customer support chatbot on your company specific documents emails and technical support tickets helping it to respond more effectively to questions about your organization products and services icl allows users to give context to the model using private data enhancing its performance in specific domains this is simple way to help language models understand and generate text that contextually relevant to specific tasks scenarios and domains augmenting text retrieval rag leverages pretrained language model in conjunction with large scale vector search system to enhance the content generation process it addresses some of the common problems generative ai and llms for dummies snowflake special edition these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited associated with gen ai systems such as limited knowledge of arcane subjects failure to recognize facts and events that took place after the model was trained and lack of insight into any proprietary data rag accesses up to date information by retrieving relevant data stored as vectors numerical representation of the data for fast retrieval such as current news to bring the model up to date with recent events or domain specific content from par ticular industry or market you can augment llms by allowing them to access your data and documents including private wikis and knowledge bases by retrieving this additional information models can produce more accurate and contextually appropriate responses for example you might use rag to generate purchase recommendations in chat by allowing it to retrieve informa tion on customer stated preferences and purchase history enabling more personalized interactions fine tuning language models fine tuning enables you to adapt an llm to particular tasks by updating the parameters of pretrained model these techniques empower users to shape llms according to their preferences and achieve better results in various applications adjusting model parameters fine tuning allows you to adjust model parameters to achieve better results there are three basic steps to the process select the pretrained llm that is most germane to the use case identify data sets related to the use case to refine the llm teach the model how to respond based on training data set that includes examples of prompts as well as the data the model needs to answer or complete the prompt in this process the model weights are adjusted to get better at generating responses to the new set of prompts evaluate the fine tuned llm to verify results meet requirements you can adjust the learning rate batch size and other factors to improve outcomes chapter llm app project lifecycle these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited by applying fine tuning techniques you can adapt model to specific needs and use cases you can also rapidly build custom ized solutions by building on top of an existing foundation model rather than training new model from scratch reinforcement learning reinforcement learning from human feedback rlhf is form of fine tuning that you can use to guide the learning process and further enhance the performance and behavior of your model with the goal of improving its responses over time many creators of large language systems use this technique to teach their chat bots to carry on realistic conversations such as to engage in dialogue rather than just provide one off responses you can use rlhf to train your models to better understand human prompts and generate more humanlike responses as well as to ensure that the model aligns with your preferences rlhf can also help you minimize the risk of harmful content by train ing the model to avoid toxic language or to avoid sensitive topics tuning models to learn individual preferences opens the door to exciting new applications of gen ai technology for example in the business world you can create personalized assistants in the field of education you can develop tailored learning plans that meet the unique needs of each student in healthcare you can leverage patient data and clinical expertise to create personalized treatment plans resulting in more effective and precise medical interventions in the entertainment industry you can use rlhf to generate personalized recommendations for users enhancing their viewing or listening experiences rlhf can improve model performance over the original pre trained version it can also help you to avoid potentially harmful or inaccurate language that results when models are trained on data from the internet where such language is common using vector database llms use vector embeddings to represent textual elements with each word mapped to token id and transformed into vector for more on this see chapter these mathematical represen tations enable efficient storage and searching of data as well as identification of semantically related text generative ai and llms for dummies snowflake special edition these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited vector database is important because it enables efficient storage retrieval and manipulation of vector embeddings by assigning unique key to each vector the database allows for quick and direct access to the content at discrete level this capability is par ticularly valuable in applications like rag where rapid retrieval and matching of vectors allow the model to discover semantically related text such as product that similar to one that cus tomer searched for previously implementing llm applications selecting and adapting an llm is an iterative process once you have model that well aligned with your needs you can deploy it to continuously run inference as stand alone service or as part of an application user interface deploying apps into containers many devops teams use containerization software such as docker to package their llm applications containers can be consistently deployed across many types of computing environments this is useful for sophisticated ai models which may have special processing needs and require access to massive amounts of pro prietary data unfortunately the complexity of creating developing and run ning ai workloads at scale forces developers and data scientists to spend their time managing containers rather than developing new applications one solution is to standardize on cloud data platform that enables you to deploy manage and scale llms and other con tainerized workloads via an infrastructure that is fully managed by the data platform itself this allows you to run llm jobs within governed environment and to take advantage of configurable hardware options such as graphics processing units gpus comprehensive cloud data platform includes scalable pool of compute resources which insulates your team from the complex ities of managing and maintaining infrastructure moving gov erned data outside of the data platform thereby exposing it to additional security risks isn necessary to use it within your ai models and applications chapter llm app project lifecycle these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited some data platforms also allow you to use data applications and models from third party providers instantly available as native apps within an associated marketplace developers and data scientists in large enterprises and other organizations must deal with massive amounts of proprietary data to run tune and deploy their models unfortunately many of these scarce technology professionals must spend their time managing compute and storage resources for these applications rather than focusing on the business problems they re trying to solve select cloud data platform that offers serverless access to llms as well as container services for running gen ai apps in fully governed easy to provision managed services environment allocating specialized hardware having the right hardware is essential for training tuning and running llm models gpus can accelerate training and inference processes ideally your cloud data platform should automatically provision these hardware resources in the background to maximize deployment flexibility select cloud data platform that is cloud agnostic which means it can work with the major cloud service providers csp that way you can abstract where the model runs and easily move it to the cloud that makes the most sense whether it because of where the data is or where the end user application is hosted integrating apps and data as discussed throughout this chapter llms and other ai models often leverage unique individualized data sets to improve the relevance and accuracy of their outputs capturing process ing storing and synchronizing data among projects can be very complex sometimes exposing your organization to data privacy violations and compliance risks by leveraging centralized cloud data platform that pro vides near unlimited data storage and compute power gen ai stakeholders can acquire the data they need to use customize and contextualize new applications quickly either using open source models as is or models fine tuned to specific data sets the plat form should empower any user to incorporate llms into analyti cal processes quickly and developers can create ai powered apps in minutes whether it uses an llm or more traditional machine generative ai and llms for dummies snowflake special edition these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited learning ml model and in the same platform teams can col laborate in executing all related tasks such as data preparation data engineering analytics and other important workloads see figure leading cloud data platform vendors offer tools to build gen ai applications such as chatbots and search engines using the neces sary building blocks for llm app development that come natively integrated they may also offer tools for ingesting and querying documents such as loading legal contracts invoices rental agree ments and many other types of content then use the reasoning engine within the llm to instantly extract meaning from them if the data platform includes marketplace you can avail yourself of gen ai apps from other data platform users as well as package your own ai models and distribute them as applications to other marketplace participants figure cloud data platform that supports many data types and spans public clouds can unite the work of data analysts data scientists and data engineers chapter bringing llm apps into production these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited chapter in this chapter understanding semantic caching feature injection and context retrieval exploring processing for inference developing interactive applications with user friendly interfaces orchestrating ai agents splitting and chaining prompts bringing llm apps into production you progress beyond using model out of the box with out any customization and begin incorporating custom data into your language models you will most likely need to assess your data processing needs this chapter discusses the challenges of bringing large language model llm apps into pro duction including building data pipelines improving model accuracy calculating costs orchestrating external data sources developing user interfaces and calling external functions adapting data pipelines data pipelines play critical role in gen ai initiatives by facilitat ing the smooth flow of data including efficient data ingestion preprocessing training and inference by establishing robust data pipelines data engineers can ensure continuous and reliable supply of high quality data which is vital for training accurate and effective models generative ai and llms for dummies snowflake special edition these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited by integrating your gen ai initiatives into your existing data infrastructure you can often avoid building data pipelines and other foundational services for more on this see chapter cloud data platform provides scalable gpu infrastructure and vector search functionality as well as flexibility to reuse and adapt existing data pipelines developed for other downstream processes such as business intelligence analytics and machine learning ml without causing bottlenecks in addition to the traditional data pipelines where data is cleaned curated and governed you need to pay attention to semantic caching feature injection and context retrieval these are integral components of data pipelines that feed the llms because they enhance the performance personalization and accuracy of ai models semantic caching semantic caching involves temporarily storing semantic represen tations or embeddings of data by employing semantic caching techniques ai systems can provide more precise meaningful and efficient responses for example let say that you have chatbot that needs to generate responses to user queries before the bot goes live it can precompute the semantic representations of large set of possible user queries and store them in cache these representations capture the underlying meaning or intent of the queries when user interacts with the chatbot instead of processing the query from scratch the system can retrieve the precomputed semantic representation from the cache this sig nificantly reduces the computational overhead and speeds up the response generation process semantic caching allows ai systems to generate responses more quickly and efficiently by having relevant data easily accessible for computations it can be particularly useful in scenarios where real time or near real time responses are required such as in chatbots virtual assistants or recommendation systems feature injection feature injection refers to the process of incorporating additional information or features into the ai model although feature engi neering is general practice in data science feature injection is specialized technique used to enhance the performance and chapter bringing llm apps into production these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited reasoning capabilities of ai models features can improve the model ability to handle specific tasks by injecting relevant fea tures the model can capture and leverage important patterns in the data leading to improved performance by introducing features that are relevant to specific prompt llms can gain deeper understanding of the data and can better capture complex patterns and relationships context retrieval context retrieval involves retrieving relevant contextual informa tion to enhance the understanding of ai models by considering the surrounding context such as previous interactions or user history ai systems can generate more accurate and personal ized completions for example customer support system might use context retrieval to provide personal assistance if customer has previously interacted with the support system and mentioned specific issue or order number the system can retrieve that context to better understand the customer current inquiry or concern retrieval augmented generation rag type of in context learning icl is also important in this context see chapter for additional information processing for inference processing for inference involves running the necessary computa tions to apply trained model to new data and generate predic tions or outcomes for example if you re creating generative ai gen ai app to quickly analyze hundreds of contracts to identify areas of business risk you need to help the model learn what types of language or clauses to spend more time on the app that sits on top of the model becomes the interface for the legal and risk teams to either adjust the contracts or find ways to mitigate risks on existing agreements there are three primary considerations infrastructure this involves selecting suitable hardware infrastructure gpus are specifically designed to handle parallel computations making them well suited for running llms efficiently the choice of gpu depends on factors such as the model size memory requirements and latency constraints generative ai and llms for dummies snowflake special edition these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited access the model can be hosted either on private servers or cloud platforms hosting in the cloud offers benefits such as scalability easy deployment and maintenance however this choice raises data security and governance concerns emphasizing the need for proper measures to protect sensitive data and ensure compliance with relevant regula tions cloud data platform can alleviate these concerns consumption llms can be accessed via apis and software functions enabling programmatic integration they can also be accessed through application specific interfaces making them accessible to broader range of users the choice of access method depends on the specific use case target audience and the level of flexibility desired reducing latency latency refers to the time it takes the llm to make predictions once it receives input data this is an important consideration for gen ai projects that require real time responses for exam ple if you re creating customer support chatbot low latency is crucial to provide quick and efficient responses to customer inquiries enabling real time interactions as described through out this book keeping the processing close to the data is key strat egy for reducing the latency of gen ai applications in production environment it also allows you to reduce the amount of data that needs to be transferred between the compute resources and the storage layer improving performance while reducing costs and data security risks various factors can impact ai performance including the com plexity or size of your model the amount of input data and the network latency between the processing and data storage lay ers to reduce latency and improve overall performance consider using smaller models optimizing models for inference using efficient hardware and software and keeping the processing close to the data this strategy can also improve the scalability of gen ai applications by distributing the processing across multiple compute resources you can scale these systems to handle larger volumes of data and more complex workloads all while reducing the latency of predictions chapter bringing llm apps into production these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited calculating costs the cost of using cloud data platform is typically based on three interrelated metrics data transfer volume data storage con sumption and compute resources the best data platforms sepa rate these three services to give administrators complete control over usage your data platform should make it easy to track the consumption of all cloud services this includes built in resource monitoring and management features that provide transparency into usage and billing ideally with granular chargeback capabili ties to tie usage to individual budgets departments and work groups data administrators can set guardrails to make sure no individual or workgroup spends more than expected for exam ple they can set time out periods for each type of workload along with auto suspend and auto resume features to automatically start and stop resource accounting when the platform isn processing data they may also set limits at granular level such as deter mining how long training model can run before it is terminated make sure that the pricing model for your cloud data plat form matches the value you obtain from it paying for set amount of storage and computing power commonly known as subscription based pricing can cause you to incur significant costs and requires regular management to ensure that you don pay for more capacity than you need your cloud data platform should offer usage based pricing with billing in per second increments creating user interfaces the front end user interface ui of gen ai application provides users with way to input data receive output from the processing engine and control the application behavior most uis are based on some form of the following web apps are the most common type of front end for gen ai apps because they re relatively easy to develop and can be accessed from any device with web browser mobile apps tailored to specific devices such as tablets and smartphones offer more immersive and engaging experience for users they can take advantage of the unique aspects of each platform and can cache data for offline use generative ai and llms for dummies snowflake special edition these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited chat interfaces are used in gen ai apps when the app needs to converse with the user such as to answer questions or assist with certain tasks desktop apps are useful for gen ai apps that require lot of processing power or that need to access local resources command line interfaces clis are sometimes used for gen ai apps that are accessed by developers and data scientists such as to empower software engineers to generate code simplifying development and deployment complete data platform includes the necessary primitives for building and deploying gen ai applications without requiring developers to move data or application code to an external system this accelerates the process of building web apps chatbots and other front end user interfaces look for platform that offers user friendly environment for working with python and other popular coding languages characterized by the following essen tial capabilities high performance environment for interacting with llms and processing large volumes of data innate scalability to handle an escalating number of users and manage lots of concurrent requests comprehensive security so that gen ai apps can safely process sensitive data according to enterprise policies ease of use making gen ai projects accessible to users with limited programming experience by providing pre built uis and ways to interact using natural language orchestrating ai agents llms have brought wide variety of tools techniques and frameworks to the task of building ai powered applications new developer tools are emerging under the umbrella of llmops short for llm operations chapter bringing llm apps into production these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited among llmops tools orchestration frameworks can be used to coordinate ai agents and other components to accomplish specific goals ai agents are simply individual instances of language mod els that are responsible for performing specific tasks such as text summarization language translation and sentiment analysis they re coordinated and managed within an orchestration sys tem to complete complex language processing tasks this process known as orchestration involves organizing agents coordinating the input output of various models and managing the flow of data and information among agents for example in an commerce scenario chatbot interacting with customer might use ai agents to retrieve order details from database generate request for return label using shipping partner api confirm the customer information and initiate the return process by sending shipping label connecting llms to external applications enables them to engage with the wider world expanding their usefulness beyond language related tasks as demonstrated by the commerce example llms can initiate actions by interacting with apis llms can also establish connections with other programming resources such as python interpreter which allows them to incorporate precise calculations into their outputs these integra tions broaden the capabilities of llms and enable them to interact with various external applications enhancing their overall func tionality and versatility see figure figure an orchestration library is used to ensure seamless flow of data between the user application and the external assets generative ai and llms for dummies snowflake special edition these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited all gen ai applications must perform the basic function of pass ing input to the llm and returning the results or completions this is often done through an orchestration library that simplifies access to external data sources and connects to apis within other applications using retrieval augmented generation rag to connect llms to external data sources is an important tactic when you need to update the model with current information such as break ing news or new research see chapter for more on this topic however many text sources are too long to fit into the limited context window of the model which typically holds only few thousand tokens instead the external data sources are divided into chunks split each of which will fit in the context window and chained together packages such as langchain can handle this work for you orchestrating ai projects orchestration libraries simplify access to external data sources and connect to apis within other applications in number of ways including chaining together different prompts allows developers to com bine prompts to create more complex applications this is useful for tasks such as generating creative text formats answering ques tions in comprehensive way and providing assistance with tasks connecting to data sources connect to external data sources such as databases apis and files this allows developers to build applications that can access and process information from myriad enterprise sources scaling to multiple llms use to scale ai applications that have interaction among multiple llms this can become harder to maintain but can also allow developers to use specialized llms for different tasks chapter reviewing security and ethical considerations these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited chapter in this chapter mitigating data privacy concerns alleviating biases and prejudices fostering ethical training and deployment practices taking responsibility for data privacy and security respecting copyright laws reviewing security and ethical considerations lthough large language models llms can process text and create content on just about any subject it is crucial for businesses to consider issues of intellectual property data privacy and potential content misuse generative ai gen ai applications are trained on massive data sets of text and code which may include sensitive or legally protected information if this data isn properly protected it could be leaked to third par ties or accessed by unauthorized individuals this chapter discusses the ethical implications of using llms and also addresses few practical concerns generative ai and llms for dummies snowflake special edition these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited reiterating the importance of security and governance gen ai is powerful technology with the potential to revolution ize many enterprise business functions enterprises must pay attention to the data privacy risks associated with this technology and take steps to mitigate these risks choose software vendors that have proven record along with third party certifications of data privacy and security carefully review the vendor terms of service and privacy policy to understand how your data will be used appoint data steward ideally business owner who understands the data to take charge of each data set establish consistent procedures for data security data privacy and data governance to satisfy industry regulations and avoid compliance violations during development and production continually monitor and audit gen ai apps to identify and mitigate any potential risks this may include monitoring the outputs of these applica tions for sensitive information and regularly reviewing the training data to ensure that it is relevant and up to date take advantage of cloud data platform that allows you to break down silos and extend consistent governance and security to dis parate sources internally from your enterprise applications and externally from business partners and third party data providers the platform should support popular programming languages scalable gpu infrastructure and provide flexibility to use open source tools to maximize options for your team chapter reviewing security and ethical considerations these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited centralizing data governance one of the primary reasons to use cloud data platform for your gen ai initiatives is because it allows you to centralize data privacy and protect sensitive customer information enterprises should implement appropriate data privacy and security measures which may include measures such as data encryption access control intrusion detection and comprehensive data governance data governance entails knowing precisely what data you have where it resides who is authorized to access it and how each type of user is permitted to use it instituting comprehensive controls reduces the risk of compliance violations all data governance strategies should seek to protect sensitive data as it is accessed shared and exchanged across the organization and beyond all of this should apply not only when data is stored but also when processed by model or surfaced in an application mitigating data privacy concerns pay attention to these data privacy concerns when developing deploying and using gen ai applications unintentional disclosure of sensitive information gen ai apps can sometimes generate outputs that contain sensitive customer information even if the prompts or inputs don explicitly mention this information for example gen ai application used to gener ate marketing copy could generate text that contains customer names and addresses even if the prompt only specifies the product or service being advertised misuse of generated data gen ai applications can be used to generate synthetic data that indistinguishable from real data this synthetic data could then be used for malicious purposes such as identity theft or fraud it could also be used to create deep fakes or other forms of disinformation compliance violations data privacy regulations such as the global data protection act gdpr and california consumer protection act ccpa impose strict requirements on how businesses can collect use and store personal data these same regulations apply to data used in the training of gen ai models generative ai and llms for dummies snowflake special edition these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited alleviating biases one important ethical consideration involves being alert to the inherent model biases that may be present in the training data which may cause llms to generate outputs that are discrimina tory or unfair for example if historical data set contains biases against certain demographics such as race or gender an llm trained on this data may inadvertently perpetuate those biases if marketing team asks an llm to generate content for customer of specific gender it important to keep in mind what kind of bias the model may have as it creates that content even if there is no explicit intent to discriminate ai enthusiasts commonly cite the three hs when discussing the responsible deployment of ai helpfulness honesty and harmlessness acknowledging open source risks open source llms such as llama bert and falcon offer tre mendous capabilities at little or no cost to users but they can come with risks that are part of the model training data set which is often not publicly accessible other open source tools that can be used to build llm apps such as an orchestration framework vector database and so on may be vulnerable to risks if not regularly updated and patched without proper security measures and consistent main tenance practices malicious entities can sometimes exploit these vulnerabilities in addition to these security concerns open source offerings may exhibit inconsistent quality due to the basic nature of their development they re the byproduct of many community contri butions consider the cost performance and compliance risks of using any llm the choice between open source and proprietary llms depends on your organization specific needs technical resources and risk tolerance see chapter for more chapter reviewing security and ethical considerations these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited contending with hallucinations as demonstrated throughout this book llms have an uncanny capability to engage in dialogue answer questions provide expla nations generate creative text and assist with various language related tasks however it important to note that while llms often exhibit impressive capabilities they may occasionally pro duce incorrect or nonsensical responses they are also known to hallucinate meaning that they may generate content that is fictional or erroneous mitigating hallucinations involves implementing the strategies discussed in chapters and fine tuning the model using relia ble and accurate data incorporating human review and oversight and continuously monitoring and refining gen ai systems to min imize the occurrence of false or misleading information enforcing ethical practices when developing and training gen ai models follow these three principles bias mitigation llms can reflect and reinforce societal biases present in the data on which they re trained ethical considerations involve identifying and mitigating biases to ensure fair and equitable outcomes developers and users should actively work to minimize biased results and ensure models that are inclusive and representative responsible use establish guidelines and guardrails to prevent misuse or harmful applications of llms this includes setting boundaries and restrictions on the use of llms to avoid the spread of misinformation hate speech or other forms of harmful content societal impact llms have the potential to influence public discourse and shape attitudes ethical considerations involve understanding the broader societal impact of using llms and considering the potential consequences for various stakeholders generative ai and llms for dummies snowflake special edition these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited observing copyright laws in september john grisham jodi picoult jonathan franzen george martin and other authors joined the authors guild in filing class action suit against openai alleging that the com pany gpt technology is illegally using the writers copyrighted works the complaint called the usage of these works by llms flagrant and harmful copyright infringement claiming that their books were misused in the training of its artificial intelligences this suit may have far reaching consequences for openai and other llm vendors depending on how the litigation progresses comedians writers musicians movie studios and many other content creators have filed similar lawsuits alleging that their original works are copyright protected and may not be freely used to train llms without permission authors should have the right to decide when their works are used to train ai stated jonathan franzen in september press release issued by the authors guild if they choose to opt in they should be appro priately compensated these types of cases highlight the importance of respecting copy righted content that may have been used to train foundation models legal and regulatory frameworks including litigation outcomes will help the ai industry establish clear guidelines and reinforce important ethical norms to avoid further legal actions in the future in the meantime enterprises should be aware of the implications of the applications they create and the content they use in all gen ai endeavors chapter five steps to generative ai these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited chapter five steps to generative ai ollowing the steps in this chapter will help ensure you reap positive new levels of productivity identify business problems rank potential projects based on expected business impact data readiness and level of executive sponsorship research and evaluate pretrained language models minimize complexity of infrastructure maintenance and consider solutions that empower large numbers of users to derive value from data select data platform how do you make sure your data is secured and governed from the time it used to fine tune until it is presented through the app ui how easy is it to allocate and scale gpus standardize on cloud data platform that offers these benefits scalable pay as you go infrastructure to handle the storage and computational requirements near zero maintenance there no need to perform platform updates or other administrative tasks generative ai and llms for dummies snowflake special edition these materials are john wiley sons inc any dissemination distribution or unauthorized use is strictly prohibited access large language model llm app stack primitives that help teams build custom solutions without integrations of multiple platforms capability for those without ai expertise to bring gen ai to their daily workflows with ui driven experiences access to structured semistructured unstructured data both internal and from third parties via marketplace native support for popular ai frameworks tools and programming languages build data foundation consolidate your data to remove silos create data pipelines and make sure that all data is consistently cleansed establish consis tent procedures for data privacy and data governance to satisfy industry regulations extend those procedures to data and apps from third party providers lastly minimize data exfiltration into compute environments that don apply consistent security and governance policies of the data create culture of collaboration how do you enable data scientists analysts developers and busi ness users to access the same data sets simultaneously without having to copy or move the data make sure your data platform empowers all pertinent stakeholders to easily collaborate as they share data models and applications educate business users on prompt engineering and other ways to leverage models without customizations that require deeper ai expertise measure learn celebrate how do you gauge the success of your gen ai initiatives start small experiment identify metrics to demonstrate busi ness results and validate progress with executive sponsors and stakeholders share best practices and encourage reusability strive to democratize gen ai capabilities throughout your entire organization wiley end user license agreement go to www wiley com go eula to access wiley ebook eula